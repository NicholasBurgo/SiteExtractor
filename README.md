# Universal Site Extractor v2

A clean, production-ready fullstack site extractor with FastAPI backend, React frontend, and comprehensive content extraction capabilities.

## âœ¨ What's New in v2

This is a **complete rewrite** that replaces the old extraction system with:

- **FastAPI Backend**: High-performance async API with automatic Swagger documentation
- **React Frontend**: Modern UI with real-time progress updates and advanced filtering  
- **Multi-format Support**: HTML, PDF, DOCX, JSON, CSV, and image extraction
- **JavaScript Rendering**: Optional Playwright integration for JS-heavy sites
- **Smart Extraction**: Uses readability, trafilatura, and custom extractors
- **Deduplication**: SimHash-based near-duplicate detection
- **Rate Limiting**: Polite crawling with robots.txt compliance
- **Real-time Monitoring**: Live progress updates and statistics
- **Confirmation Workflow**: Review and edit extracted data before seeding
- **Seed Generation**: Export generator-ready JSON for site building

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.10+** (for FastAPI backend)
- **Node.js 18+** (for React frontend)
- **Git** (for cloning)

### Option 1: Development Mode (Recommended)

1. **Start both servers**:
   ```bash
   # On Fedora Linux (recommended)
   ./start-fedora.sh
   
   # Or on any Linux/Mac
   chmod +x scripts/dev.sh
   ./scripts/dev.sh
   
   # On Windows
   scripts\dev.bat
   ```
   
   The script will automatically:
   - Check and install Python 3 and Node.js if needed (Fedora only)
   - Create a Python virtual environment if it doesn't exist
   - Upgrade pip to latest version
   - Install backend dependencies in the virtual environment
   - Install frontend dependencies
   - Start both frontend and backend servers

2. **Access the application**:
   - **Frontend UI**: http://localhost:5173
   - **Backend API**: http://localhost:5051  
   - **Swagger Docs**: http://localhost:5051/docs

### Option 1b: Run Services Separately (Better for Debugging)

For better visibility of logs and easier debugging, run each service in its own terminal:

**Terminal 1 - Backend:**
```bash
./start-backend.sh
```

**Terminal 2 - Frontend:**
```bash
./start-frontend.sh
```

This allows you to:
- See backend logs without frontend noise
- Restart one service without affecting the other
- Debug issues more easily

### Option 2: Manual Setup

1. **Create and activate virtual environment** (recommended):
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

2. **Backend**:
   ```bash
   cd backend
   pip install -r requirements.txt
   uvicorn app:app --reload --port 5051
   ```

3. **Frontend** (in new terminal):
   ```bash
   cd frontend
   npm install
   npm run dev
   ```

### Option 3: Docker

```bash
docker-compose up --build
```

### Production Build

```bash
# On Linux/Mac
chmod +x scripts/build.sh
./scripts/build.sh

# On Windows
scripts\build.bat
```

**Note**: The build script will automatically create a virtual environment if it doesn't exist.

## ğŸ“– How to Use

### 1. Starting an Extraction

1. **Open the web interface**: http://localhost:5173
2. **Enter target URL**: e.g., `https://example.com`
3. **Click "Start Run"**: The system will begin crawling
4. **Monitor progress**: Real-time updates in the left panel

### 2. Exploring Results

The interface has **three panels**:

- **Left Panel**: Run controls and progress summary
- **Center Panel**: Page table with filtering options  
- **Right Panel**: Detailed page preview

### 3. Filtering and Search

- **Search**: Type in the search box to find pages by title/content
- **Content Type**: Filter by HTML, PDF, DOCX, JSON, CSV, Images
- **Min Words**: Show only pages with minimum word count
- **Status**: View successful pages vs. errors

### 4. Confirmation and Seeding

After extraction completes, you can review and edit the data:

1. **Navigate to Confirmation**: Click "Confirm" on any completed run
2. **Review Prime Data**: Edit navigation structure and footer content
3. **Edit Page Content**: Modify titles, descriptions, media alt text, and links
4. **Export Seed**: Generate generator-ready JSON for site building

The confirmation interface provides:
- **Prime Tab**: Navigation and footer editing with drag-and-drop reordering
- **Content Tab**: Per-page editing of media, files, words, and links
- **Summary Tab**: Overview statistics and content quality recommendations

### 5. Page Details

Click any page in the table to see:
- **Full text content** (first 5000 characters)
- **Metadata** (title, author, creation date, etc.)
- **Headings** structure
- **Images** and links found
- **Statistics** (word count, image count, etc.)

## âš™ï¸ Configuration

Copy `env.example` to `.env` and customize:

```bash
cp env.example .env
```

### Key Settings

| Setting | Default | Description |
|---------|---------|-------------|
| `GLOBAL_CONCURRENCY` | 12 | Number of concurrent requests |
| `PER_HOST_LIMIT` | 6 | Requests per host (politeness) |
| `REQUEST_TIMEOUT_SEC` | 20 | Request timeout in seconds |
| `MAX_PAGES_DEFAULT` | 400 | Default page limit |
| `RENDER_ENABLED` | false | Enable JavaScript rendering |
| `RENDER_BUDGET` | 0.10 | Percentage of pages to render with JS |

## ğŸ“ Project Structure

```
site_extractor_v2/
â”œâ”€â”€ backend/                    # FastAPI Backend
â”‚   â”œâ”€â”€ app.py                 # Main FastAPI application
â”‚   â”œâ”€â”€ routers/               # API route handlers
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ runs.py           # Run management endpoints
â”‚   â”‚   â”œâ”€â”€ pages.py          # Page listing/details endpoints
â”‚   â”‚   â”œâ”€â”€ review.py         # Review and aggregation endpoints
â”‚   â”‚   â””â”€â”€ confirm.py        # Confirmation workflow endpoints
â”‚   â”œâ”€â”€ core/                 # Core configuration and types
â”‚   â”‚   â”œâ”€â”€ config.py         # Settings and environment config
â”‚   â”‚   â”œâ”€â”€ deps.py           # Dependency injection
â”‚   â”‚   â””â”€â”€ types.py          # Pydantic models
â”‚   â”œâ”€â”€ crawl/                # Crawling engine
â”‚   â”‚   â”œâ”€â”€ runner.py         # Main orchestration
â”‚   â”‚   â”œâ”€â”€ frontier.py       # URL queue management
â”‚   â”‚   â”œâ”€â”€ fetch.py          # Async HTTP client
â”‚   â”‚   â”œâ”€â”€ render_pool.py    # Playwright browser pool
â”‚   â”‚   â””â”€â”€ robots.py         # Robots.txt compliance
â”‚   â”œâ”€â”€ extract/              # Content extractors
â”‚   â”‚   â”œâ”€â”€ html.py           # HTML content extraction
â”‚   â”‚   â”œâ”€â”€ pdfs.py           # PDF text extraction
â”‚   â”‚   â”œâ”€â”€ docx_.py          # DOCX document extraction
â”‚   â”‚   â”œâ”€â”€ json_csv.py       # JSON/CSV data extraction
â”‚   â”‚   â”œâ”€â”€ images.py         # Image metadata extraction
â”‚   â”‚   â”œâ”€â”€ nav_footer.py     # Navigation and footer extraction
â”‚   â”‚   â””â”€â”€ files_words_links.py # Structured content extraction
â”‚   â”œâ”€â”€ storage/              # Data storage
â”‚   â”‚   â”œâ”€â”€ runs.py           # File-based run storage
â”‚   â”‚   â”œâ”€â”€ simhash.py        # Near-duplicate detection
â”‚   â”‚   â”œâ”€â”€ confirmation.py   # Confirmation data storage
â”‚   â”‚   â””â”€â”€ seed.py           # Seed generation utilities
â”‚   â”œâ”€â”€ Dockerfile            # Backend container config
â”‚   â”œâ”€â”€ requirements.txt      # Python dependencies
â”‚   â””â”€â”€ README.md            # Backend documentation
â”œâ”€â”€ frontend/                  # React Frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx          # Main application component
â”‚   â”‚   â”œâ”€â”€ main.tsx         # React entry point
â”‚   â”‚   â”œâ”€â”€ styles.css       # Tailwind CSS imports
â”‚   â”‚   â”œâ”€â”€ components/      # Reusable UI components
â”‚   â”‚   â”‚   â”œâ”€â”€ TopBar.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ RunSummary.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ RunFilters.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ RunTable.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ PageDetail.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ PrimeTabs.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ContentTabs.tsx
â”‚   â”‚   â”‚   â””â”€â”€ SummaryTab.tsx
â”‚   â”‚   â”œâ”€â”€ lib/             # Utilities and types
â”‚   â”‚   â”‚   â”œâ”€â”€ api.ts       # API client functions
â”‚   â”‚   â”‚   â”œâ”€â”€ types.ts     # TypeScript interfaces
â”‚   â”‚   â”‚   â”œâ”€â”€ api.confirm.ts # Confirmation API client
â”‚   â”‚   â”‚   â””â”€â”€ types.confirm.ts # Confirmation types
â”‚   â”‚   â””â”€â”€ pages/           # Page components
â”‚   â”‚       â”œâ”€â”€ Generator.tsx
â”‚   â”‚       â”œâ”€â”€ Review.tsx
â”‚   â”‚       â”œâ”€â”€ RunView.tsx
â”‚   â”‚       â””â”€â”€ ConfirmPage.tsx
â”‚   â”œâ”€â”€ Dockerfile           # Frontend container config
â”‚   â”œâ”€â”€ index.html           # HTML template
â”‚   â”œâ”€â”€ package.json         # Node.js dependencies
â”‚   â”œâ”€â”€ tailwind.config.js   # Tailwind CSS config
â”‚   â”œâ”€â”€ tsconfig.json        # TypeScript config
â”‚   â””â”€â”€ vite.config.ts      # Vite build config
â”œâ”€â”€ scripts/                  # Development scripts
â”‚   â”œâ”€â”€ dev.sh              # Linux/Mac dev startup
â”‚   â”œâ”€â”€ dev.bat             # Windows dev startup
â”‚   â”œâ”€â”€ build.sh            # Linux/Mac build script
â”‚   â””â”€â”€ build.bat           # Windows build script
â”œâ”€â”€ runs/                    # Generated extraction data
â”œâ”€â”€ docker-compose.yml       # Multi-service Docker setup
â”œâ”€â”€ .env.example            # Environment configuration template
â”œâ”€â”€ LICENSE                 # MIT License
â””â”€â”€ README.md              # This file
```

## Architecture

### Backend (FastAPI)

- **Crawler**: Async HTTP client with rate limiting and retry logic
- **Extractors**: Content-type specific extraction modules
- **Storage**: File-based storage with JSON serialization
- **Deduplication**: SimHash for near-duplicate detection
- **Rendering**: Optional Playwright pool for JavaScript pages

### Frontend (React + TypeScript)

- **Real-time Updates**: Live progress monitoring
- **Advanced Filtering**: Multi-criteria page filtering
- **Responsive Design**: Mobile-friendly interface
- **Virtualization**: Handle large datasets efficiently

## API Reference

### Core Endpoints

- `POST /api/runs/start` - Start new extraction
- `GET /api/runs/{run_id}/progress` - Get run progress
- `GET /api/runs/{run_id}/pages` - Get paginated pages
- `GET /api/runs/{run_id}/page/{page_id}` - Get page details
- `POST /api/runs/{run_id}/stop` - Stop running extraction

### Confirmation Endpoints

- `GET /api/confirm/{run_id}/prime` - Get navigation, footer, and pages index
- `GET /api/confirm/{run_id}/content?page_path={path}` - Get structured page content
- `PATCH /api/confirm/{run_id}/prime/nav` - Update navigation structure
- `PATCH /api/confirm/{run_id}/prime/footer` - Update footer content
- `PATCH /api/confirm/{run_id}/content?page_path={path}` - Update page content
- `POST /api/confirm/{run_id}/seed` - Generate seed.json for site building

### Content Types Supported

- **HTML**: Full text extraction with metadata, links, images
- **PDF**: Text extraction with page count and metadata
- **DOCX**: Document text with heading structure
- **JSON/CSV**: Schema inference and sample data
- **Images**: Metadata extraction (size, format, EXIF)

## Configuration

Copy `env.example` to `.env` and customize settings:

```bash
cp env.example .env
```

Key settings:
- `GLOBAL_CONCURRENCY`: Number of concurrent requests
- `PER_HOST_LIMIT`: Requests per host
- `MAX_PAGES_DEFAULT`: Default page limit
- `RENDER_ENABLED`: Enable JavaScript rendering
- `RENDER_BUDGET`: Percentage of pages to render with JS

## Usage

### Starting an Extraction

1. Open the web interface
2. Enter the target URL
3. Configure settings (max pages, depth, concurrency)
4. Click "Start Run"

### Monitoring Progress

- View real-time progress in the run summary
- Monitor queued, visited, and error counts
- Check per-host statistics
- View estimated time remaining

### Exploring Results

- **Filter pages**: By content type, word count, errors, etc.
- **Search**: Find specific pages by title or content
- **Sort**: By word count, images, links, or processing time
- **Details**: Click any page to view full content and metadata

## Advanced Features

### JavaScript Rendering

Enable Playwright for JavaScript-heavy sites:

```bash
pip install playwright
playwright install chromium
```

Set `RENDER_ENABLED=true` in your `.env` file.

### Custom Extractors

Extend the system with custom content extractors:

```python
from backend.extract.base import BaseExtractor

class CustomExtractor(BaseExtractor):
    async def extract(self, url, content, headers):
        # Your extraction logic
        return PageResult(...)
```

### Rate Limiting

Configure per-domain rate limits:

```python
# In your configuration
PER_HOST_LIMIT = 6  # requests per host
REQUESTS_PER_SECOND = 2.0  # global rate limit
```

## Troubleshooting

### Common Issues

1. **Import errors**: Ensure all dependencies are installed
2. **Port conflicts**: Change ports in `scripts/dev.sh`
3. **Memory issues**: Reduce `GLOBAL_CONCURRENCY` for large sites
4. **Timeout errors**: Increase `REQUEST_TIMEOUT_SEC`

### Debug Mode

Enable debug logging:

```bash
export LOG_LEVEL=DEBUG
```

### Performance Tuning

For large sites (500+ pages):
- Increase `GLOBAL_CONCURRENCY` to 20-30
- Set `PER_HOST_LIMIT` to 10-15
- Use SSD storage for better I/O performance
- Consider running on multiple machines

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## License

MIT License - see LICENSE file for details.

## Support

- Issues: GitHub Issues
- Documentation: This README
- API Docs: http://localhost:5051/docs (when running)